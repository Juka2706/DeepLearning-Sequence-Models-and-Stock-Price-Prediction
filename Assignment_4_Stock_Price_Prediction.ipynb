{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Advanced_Stock_Price_Prediction\n",
    "\n",
    "This notebook implements and compares two sequential models for stock price prediction using PyTorch, as per the assignment instructions.\n",
    "\n",
    "**Models:**\n",
    "1.  **Model 1:** A standard LSTM (Long Short-Term Memory) network.\n",
    "2.  **Model 2:** An LSTM network enhanced with an attention mechanism.\n",
    "\n",
    "**Objective:**\n",
    "Predict the *relative change* (log returns) of a stock's price and evaluate the models' performance using autoregressive prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports\n",
    "\n",
    "First, we import all necessary libraries and set up our environment, including the device (GPU if available) and key parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create a 'models' directory if it doesn't already exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# --- Configuration Parameters ---\n",
    "STOCK_TICKER = \"^GSPC\"      # S&P 500 Index\n",
    "START_DATE = \"2014-01-01\"   # 10 years of data\n",
    "END_DATE = \"2024-11-01\"     # Today's date or recent\n",
    "\n",
    "TRAIN_SPLIT = 0.85          # 85% for training\n",
    "BASE_WINDOW_SIZE = 50       # Initial window size (50 days)\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100                # Max epochs (will use early stopping)\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "PATIENCE = 10               # For early stopping\n",
    "EPOCHS_ATTENTION = 100\n",
    "LEARNING_RATE_ATTENTION = 0.0001\n",
    "PATIENCE_ATTENTION = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition\n",
    "\n",
    "We use the `yfinance` package to download the daily stock price data for our chosen ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Download data\n",
    "    # --- CHANGE ---\n",
    "    # Add auto_adjust=False to ensure 'Adj Close' is a separate column\n",
    "    data = yf.download(STOCK_TICKER, start=START_DATE, end=END_DATE, auto_adjust=False)\n",
    "    # --- END CHANGE ---\n",
    "    \n",
    "    if data.empty:\n",
    "        print(f\"No data found for ticker {STOCK_TICKER}.\")\n",
    "    else:\n",
    "        # Now 'Adj Close' and 'Volume' are guaranteed to be there\n",
    "        prices_df = data[['Adj Close', 'Volume']]\n",
    "        \n",
    "        # Plot the 'Adj Close' price\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        prices_df['Adj Close'].plot()\n",
    "        plt.title(f\"{STOCK_TICKER} Adjusted Closing Price (Absolute)\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1. Calculate Relative Values (Log Returns)\n",
    "\n",
    "As instructed, we will predict relative values instead of absolute prices. We'll use **log returns**, which are defined as $\\log(P_t / P_{t-1})$. This transformation stabilizes the variance and makes the time series more stationary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.2. Create Sliding Window Dataset\n",
    "\n",
    "We implement a function to create our dataset based on the sliding window approach.\n",
    "-   **Input (X):** A sequence of `window_size` consecutive log returns.\n",
    "-   **Target (y):** The *next* log return immediately following the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. Define Sliding Window Function (Same as before) ---\n",
    "def create_sliding_windows(data, window_size):\n",
    "    \"\"\"\n",
    "    Creates sliding window sequences (X) and corresponding targets (y).\n",
    "    Input `data` is expected to be a 2D NumPy array (N, num_features).\n",
    "    The target `y` will be the *first feature* (log_return) at the next time step.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(data.shape[0] - window_size):\n",
    "        # Input sequence: (window_size, num_features)\n",
    "        X.append(data[i : i + window_size, :])\n",
    "        \n",
    "        # Target value: (1,) - just the log return\n",
    "        # Since `data` is scaled, this `y` is ALSO scaled.\n",
    "        y.append(data[i + window_size, 0]) \n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- 2. Calculate Features ---\n",
    "\n",
    "# Feature 1: Log Returns for 'Adj Close'\n",
    "log_returns = np.log(prices_df['Adj Close'] / prices_df['Adj Close'].shift(1)).squeeze(\"columns\")\n",
    "\n",
    "# Feature 2: Log of Volume (add 1 to prevent log(0) errors)\n",
    "log_volume = np.log(prices_df['Volume'] + 1).squeeze(\"columns\")\n",
    "\n",
    "# Combine into a new DataFrame\n",
    "features_df = pd.DataFrame({\n",
    "    'log_returns': log_returns,\n",
    "    'log_volume': log_volume\n",
    "})\n",
    "\n",
    "# Drop the first row with NaN\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "# *** IMPORTANT: Save the unscaled log returns (as a pd.Series) for reconstruction ***\n",
    "log_returns_unscaled_pd = features_df['log_returns']\n",
    "\n",
    "# Convert features to a NumPy array (N, 2)\n",
    "features_np = features_df.values.astype(np.float32)\n",
    "\n",
    "# --- 3. Split the Data ---\n",
    "split_idx_raw = int(len(features_np) * TRAIN_SPLIT)\n",
    "train_data_raw = features_np[:split_idx_raw]\n",
    "test_data_raw = features_np[split_idx_raw:]\n",
    "\n",
    "# --- 4. Scale the Data ---\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data_raw)\n",
    "test_data_scaled = scaler.transform(test_data_raw)\n",
    "\n",
    "# --- 5. Extract Scaling Params for y ---\n",
    "# We still need these for un-scaling predictions later\n",
    "log_return_scale = scaler.scale_[0]\n",
    "log_return_mean = scaler.mean_[0]\n",
    "\n",
    "# --- 6. Create Sliding Windows ---\n",
    "# X will be (N, window_size, 2)\n",
    "# y_train and y_test are ALREADY SCALED because they come from train_data_scaled\n",
    "X_train, y_train = create_sliding_windows(train_data_scaled, BASE_WINDOW_SIZE)\n",
    "X_test, y_test = create_sliding_windows(test_data_scaled, BASE_WINDOW_SIZE)\n",
    "\n",
    "# --- 7. Reshape y ---\n",
    "# (The buggy manual scaling section is now REMOVED)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"Training set: X shape {X_train.shape}, y shape {y_train.shape}\")\n",
    "print(f\"Test set:     X shape {X_test.shape}, y shape {y_test.shape}\")\n",
    "\n",
    "# --- 8. Convert to Tensors ---\n",
    "X_train_t = torch.tensor(X_train).float()\n",
    "y_train_t = torch.tensor(y_train).float()\n",
    "X_test_t = torch.tensor(X_test).float()\n",
    "y_test_t = torch.tensor(y_test).float()\n",
    "\n",
    "print(f\"\\nTensor shapes: {X_train_t.shape}\")\n",
    "\n",
    "# --- 9. Create DataLoaders ---\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Model Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Model 1: LSTM Only\n",
    "\n",
    "This is a standard LSTM model. It consists of an `nn.LSTM` layer followed by an `nn.Linear` layer to produce the final prediction.\n",
    "\n",
    "**Architecture:**\n",
    "1.  Input `x` (batch, seq_len, 1) goes into the LSTM.\n",
    "2.  The LSTM outputs hidden states for all time steps.\n",
    "3.  We take the output from the *very last time step* (`lstm_out[:, -1, :]`).\n",
    "4.  This final hidden state is passed to a Linear layer to predict a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 1: LSTM Only ---\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1, dropout=DROPOUT):\n",
    "        # --- CHANGE: input_size=2 ---\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, # <-- Now 2\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape is (batch, seq_len, 2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        prediction = self.fc(last_time_step_out)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Model 2: LSTM with Attention\n",
    "\n",
    "This model adds an attention mechanism to the LSTM. Instead of just using the *last* hidden state, the attention mechanism learns to assign different \"importance\" weights to the hidden states from *all* time steps. It then computes a weighted sum (the \"context vector\") of all hidden states, which is used for the final prediction.\n",
    "\n",
    "**Architecture:**\n",
    "1.  Input `x` goes into the LSTM, producing all hidden states `lstm_out` (batch, seq_len, hidden_size).\n",
    "2.  **Attention Calculation:**\n",
    "    a.  Pass `lstm_out` through a linear layer and `tanh` activation to get \"energy\" scores.\n",
    "    b.  Pass the energy scores through another linear layer to get raw scores (batch, seq_len, 1).\n",
    "    c.  Apply `softmax` along the time dimension to get attention weights (batch, seq_len, 1). These weights sum to 1.\n",
    "3.  **Context Vector:**\n",
    "    a.  Multiply the weights with the original `lstm_out` (element-wise).\n",
    "    b.  Sum the results across the time dimension to get a single context vector (batch, hidden_size).\n",
    "4.  Pass this context vector to the final Linear layer for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 2: LSTM + Attention ---\n",
    "class LSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1, dropout=DROPOUT):\n",
    "        # --- CHANGE: input_size=2 ---\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, # <-- Now 2\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Keep the LayerNorm fix\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size) \n",
    "        self.attn_W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attn_v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape is (batch, seq_len, 2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out_norm = self.layer_norm(lstm_out) \n",
    "        \n",
    "        energy = torch.tanh(self.attn_W(lstm_out_norm))\n",
    "        scores = self.attn_v(energy)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        weighted_out = weights * lstm_out_norm\n",
    "        context = torch.sum(weighted_out, dim=1)\n",
    "        \n",
    "        prediction = self.fc(context)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 4. Training the Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 4.1. Early Stopping Utility\n",
    "\n",
    "To prevent overfitting and save time, we'll implement an `EarlyStopping` class. It monitors the validation loss and stops training if it doesn't improve for a specified number of `patience` epochs. It also saves the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0, verbose=True, path='checkpoint.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"   ...Val loss improved to {val_loss:.6f}. Saving model to {self.path}...\")\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"   ...EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 4.2. Training Function\n",
    "\n",
    "This function contains the main training and validation loop. It's reusable for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs, lr, patience, model_path):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error is suitable for regression\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    early_stopper = EarlyStopping(patience=patience, path=model_path, verbose=True)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    print(f\"--- Starting Training for {model_path} ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # --- Validation ---\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                running_val_loss += loss.item() * X_batch.size(0)\n",
    "                \n",
    "        epoch_val_loss = running_val_loss / len(test_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:03}/{epochs:03} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        early_stopper(epoch_val_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "            \n",
    "    end_time = time.time()\n",
    "    print(f\"--- Training Finished in {end_time - start_time:.2f}s ---\")\n",
    "    \n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def plot_losses(train_losses, val_losses, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 4.3. Train Model 1 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "lstm_model, lstm_train_loss, lstm_val_loss = train_model(\n",
    "    lstm_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS,\n",
    "    LEARNING_RATE,\n",
    "    PATIENCE,\n",
    "    'models/best_lstm_model.pth' # Path to save best model\n",
    ")\n",
    "\n",
    "plot_losses(lstm_train_loss, lstm_val_loss, \"Model 1: LSTM Training & Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 4.4. Train Model 2 (LSTM + Attention)\n",
    "\n",
    "We train the attention model under the same conditions (same data, epochs, lr, etc.) for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = LSTMAttentionModel(\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "attention_model, attn_train_loss, attn_val_loss = train_model(\n",
    "    attention_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS_ATTENTION,\n",
    "    LEARNING_RATE_ATTENTION,\n",
    "    PATIENCE_ATTENTION,\n",
    "    'models/best_attention_model.pth' # Path to save best model\n",
    ")\n",
    "\n",
    "plot_losses(attn_train_loss, attn_val_loss, \"Model 2: LSTM + Attention Training & Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Comparison\n",
    "\n",
    "### 5.1. Autoregressive Prediction\n",
    "\n",
    "This is the most critical part of the evaluation. We will simulate a real-world prediction scenario:\n",
    "1.  Take the **first window** from the test set (`X_test_t[0]`) as the initial input.\n",
    "2.  Predict the next value (Prediction 1).\n",
    "3.  Create a **new window** by *dropping* the first value of the initial window and *appending* Prediction 1 to the end.\n",
    "4.  Use this new window to predict the next value (Prediction 2).\n",
    "5.  Repeat this process for the entire length of the test set.\n",
    "\n",
    "This method tests the model's stability, as prediction errors will accumulate (compound) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_predict(model, initial_window_tensor, n_predictions):\n",
    "    \"\"\"\n",
    "    Performs autoregressive prediction for a multivariate (2-feature) model.\n",
    "    `initial_window_tensor` should have shape (window_size, 2).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # current_window shape: (window_size, 2)\n",
    "    current_window = initial_window_tensor.clone().to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_predictions):\n",
    "            # input_tensor shape: (1, window_size, 2)\n",
    "            input_tensor = current_window.unsqueeze(0)\n",
    "            \n",
    "            # Get prediction (pred shape will be (1, 1))\n",
    "            pred = model(input_tensor)\n",
    "            \n",
    "            # Store the prediction (as a simple number)\n",
    "            predictions.append(pred.item())\n",
    "            \n",
    "            # --- START OF FIX ---\n",
    "            \n",
    "            # We need to build a new 2-feature entry.\n",
    "            # Feature 1 is our prediction (shape (1, 1))\n",
    "            \n",
    "            # Feature 2 is the last log_volume from the current window\n",
    "            # current_window[-1, 1] gets the last vol, reshape to (1, 1)\n",
    "            last_log_volume = current_window[-1, 1].reshape(1, 1)\n",
    "            \n",
    "            # Combine them into a 2-feature tensor: (1, 2)\n",
    "            new_entry_tensor = torch.cat((pred, last_log_volume), dim=1)\n",
    "            \n",
    "            # Now concatenate the (window_size-1, 2) tensor with the new (1, 2) tensor\n",
    "            current_window = torch.cat((current_window[1:], new_entry_tensor), dim=0)\n",
    "            \n",
    "            # --- END OF FIX ---\n",
    "            \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get the first window from the test set\n",
    "# X_test_t shape is (N_test, window_size, 1). We need the first item.\n",
    "first_test_window = X_test_t[0]\n",
    "\n",
    "# Number of predictions to make == length of the test set\n",
    "n_test_predictions = len(y_test_t)\n",
    "\n",
    "print(\"Running autoregressive predictions for LSTM model...\")\n",
    "lstm_preds_autoregressive = autoregressive_predict(lstm_model, first_test_window, n_test_predictions)\n",
    "\n",
    "print(\"Running autoregressive predictions for Attention model...\")\n",
    "attn_preds_autoregressive = autoregressive_predict(attention_model, first_test_window, n_test_predictions)\n",
    "\n",
    "# Get actual values for comparison\n",
    "actuals = y_test_t.cpu().numpy().flatten()\n",
    "\n",
    "print(\"Predictions complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 5.2. Metrics\n",
    "\n",
    "We'll use common regression metrics to compare the models:\n",
    "-   **MSE (Mean Squared Error):** Penalizes large errors heavily.\n",
    "-   **RMSE (Root Mean Squared Error):** MSE in the original units (log returns).\n",
    "-   **MAE (Mean Absolute Error):** Average absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, actuals):\n",
    "    mse = mean_squared_error(actuals, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, preds)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
    "\n",
    "lstm_metrics = calculate_metrics(lstm_preds_autoregressive, actuals)\n",
    "attn_metrics = calculate_metrics(attn_preds_autoregressive, actuals)\n",
    "\n",
    "print(\"--- Autoregressive Performance Metrics ---\")\n",
    "metrics_df = pd.DataFrame([lstm_metrics, attn_metrics], index=['LSTM Model', 'Attention Model'])\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 5.3. Visualization: Predicted vs. Actual Relative Changes\n",
    "\n",
    "Let's plot the predicted log returns against the actual log returns. Due to the compounding error of autoregression, we expect the predictions to drift over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(actuals, label='Actual Log Returns', color='blue', alpha=0.7)\n",
    "plt.plot(lstm_preds_autoregressive, label='LSTM Predictions (Autoregressive)', color='orange', linestyle='--')\n",
    "plt.plot(attn_preds_autoregressive, label='Attention Predictions (Autoregressive)', color='green', linestyle=':')\n",
    "plt.legend()\n",
    "plt.title(f'Autoregressive Predictions vs Actuals (Log Returns) - Test Set')\n",
    "plt.xlabel('Time Step (in test set)')\n",
    "plt.ylabel('Log Return')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# One-Step-Ahead Prediction (For a fair model comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_step_ahead_predictions(model, data_loader): # <-- Takes data_loader as argument\n",
    "    \"\"\"\n",
    "    Performs a non-autoregressive (one-step-ahead) prediction \n",
    "    on the entire test set using the provided DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- FIX ---\n",
    "        # Use the data_loader that was passed in, NOT the global one\n",
    "        for X_batch, y_batch in data_loader: \n",
    "        # --- END FIX ---\n",
    "            \n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            \n",
    "            # Get prediction\n",
    "            batch_preds = model(X_batch)\n",
    "            \n",
    "            # Move to CPU and store\n",
    "            predictions.extend(batch_preds.cpu().numpy().flatten())\n",
    "            \n",
    "    return np.array(predictions)\n",
    "\n",
    "print(\"\\nRunning ONE-STEP-AHEAD (NON-AUTOREGRESSIVE) predictions...\")\n",
    "\n",
    "# Get one-step-ahead predictions using the MAIN test_loader\n",
    "lstm_preds_one_step = get_one_step_ahead_predictions(lstm_model, test_loader)\n",
    "attn_preds_one_step = get_one_step_ahead_predictions(attention_model, test_loader)\n",
    "\n",
    "# Get the SCALED actuals for this plot\n",
    "actuals_scaled = y_test_t.cpu().numpy().flatten()\n",
    "\n",
    "print(\"One-step-ahead predictions complete.\")\n",
    "\n",
    "# --- Visualize the One-Step-Ahead predictions ---\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(actuals_scaled, label='Actuals (Scaled Log Returns)', color='blue', alpha=0.7)\n",
    "plt.plot(lstm_preds_one_step, label='LSTM (One-Step-Ahead)', color='orange', linestyle='--')\n",
    "plt.plot(attn_preds_one_step, label='Attention (One-Step-Ahead)', color='green', linestyle=':')\n",
    "plt.legend()\n",
    "plt.title('Model Performance: One-Step-Ahead Prediction (Scaled)')\n",
    "plt.xlabel('Time Step (in test set)')\n",
    "plt.ylabel('Scaled Log Return')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Calculate Metrics for this test ---\n",
    "print(\"\\n--- One-Step-Ahead Performance Metrics (Scaled) ---\")\n",
    "lstm_metrics_one_step = calculate_metrics(lstm_preds_one_step, actuals_scaled)\n",
    "attn_metrics_one_step = calculate_metrics(attn_preds_one_step, actuals_scaled)\n",
    "\n",
    "one_step_metrics_df = pd.DataFrame([lstm_metrics_one_step, attn_metrics_one_step], \n",
    "                                   index=['LSTM (One-Step)', 'Attention (One-Step)'])\n",
    "display(one_step_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 5.4. Visualization: Reconstructed Absolute Prices\n",
    "\n",
    "To make the results more intuitive, we will reconstruct the absolute prices from the predicted log returns.\n",
    "\n",
    "The formula is: $P_t = P_{t-1} \\cdot e^{\\text{log\\_return}_t}$\n",
    "\n",
    "We need a starting price, which will be the last *actual* price *before* the test set began."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_prices(start_price, log_returns):\n",
    "    \"\"\"Converts log returns back to absolute prices.\"\"\"\n",
    "    prices = [start_price]\n",
    "    current_price = start_price\n",
    "    for log_ret in log_returns:\n",
    "        current_price = current_price * np.exp(log_ret)\n",
    "        prices.append(current_price)\n",
    "    return np.array(prices)\n",
    "\n",
    "# --- Find the correct start price ---\n",
    "\n",
    "# Use the pd.Series of unscaled log returns we saved\n",
    "first_test_log_return_index = log_returns_unscaled_pd.index[split_idx_raw + BASE_WINDOW_SIZE]\n",
    "\n",
    "# Use the 'Adj Close' column from the original prices_df\n",
    "prices_orig = prices_df['Adj Close']\n",
    "\n",
    "start_price_timestamp = prices_orig.index[prices_orig.index < first_test_log_return_index][-1]\n",
    "start_price = prices_orig.loc[start_price_timestamp].iloc[0]\n",
    "\n",
    "actual_test_prices_pd = prices_orig.loc[first_test_log_return_index:]\n",
    "\n",
    "\n",
    "# --- Un-scale the predictions (NEEDS TO HAPPEN FIRST) ---\n",
    "# We use the mean and scale we saved from the *first feature*\n",
    "lstm_preds_unscaled = (lstm_preds_autoregressive * log_return_scale) + log_return_mean\n",
    "attn_preds_unscaled = (attn_preds_autoregressive * log_return_scale) + log_return_mean\n",
    "\n",
    "\n",
    "# --- NOW we can get the length and slice our data ---\n",
    "num_preds = len(lstm_preds_unscaled)\n",
    "actual_test_prices = actual_test_prices_pd.iloc[:num_preds].values\n",
    "\n",
    "print(f\"Reconstruction Start Price (on {start_price_timestamp.date()}): ${start_price:.2f}\")\n",
    "print(f\"Actual test prices to compare: {len(actual_test_prices)}\")\n",
    "print(f\"Predictions made: {num_preds}\")\n",
    "\n",
    "# --- Reconstruct prices from un-scaled predictions ---\n",
    "lstm_recon_prices = reconstruct_prices(start_price, lstm_preds_unscaled)\n",
    "attn_recon_prices = reconstruct_prices(start_price, attn_preds_unscaled)\n",
    "\n",
    "# --- Plot reconstructed prices ---\n",
    "plt.figure(figsize=(15, 7))\n",
    "test_index = actual_test_prices_pd.iloc[:num_preds].index\n",
    "\n",
    "plt.plot(test_index, actual_test_prices, label='Actual Prices', color='blue', linewidth=2)\n",
    "plt.plot(test_index, lstm_recon_prices[1:], label='LSTM Reconstructed Prices', color='orange', linestyle='--')\n",
    "plt.plot(test_index, attn_recon_prices[1:], label='Attention Reconstructed Prices', color='green', linestyle=':')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Reconstructed Absolute Prices (Autoregressive) - {STOCK_TICKER}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 6. Experimentation with Sliding Window Sizes\n",
    "\n",
    "Now we'll automate the process to test different window sizes (10, 25, 50, 100). \n",
    "\n",
    "**Note:** As per the assignment instructions, we will **only run this for the basic LSTM model** because training the attention model repeatedly is very time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_window_experiment(window_size, full_features_np):\n",
    "    \"\"\"A complete pipeline to train and evaluate an LSTM model for a given window size.\"\"\"\n",
    "    print(f\"\\n{'='*20} RUNNING FOR WINDOW SIZE: {window_size} {'='*20}\")\n",
    "    \n",
    "    # 1. Data Prep (Done inside the loop for robustness)\n",
    "    \n",
    "    # Split\n",
    "    split_idx = int(len(full_features_np) * TRAIN_SPLIT)\n",
    "    train_raw = full_features_np[:split_idx]\n",
    "    test_raw = full_features_np[split_idx:]\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_raw)\n",
    "    test_scaled = scaler.transform(test_raw)\n",
    "    \n",
    "    # Create windows\n",
    "    X_train, y_train = create_sliding_windows(train_scaled, window_size)\n",
    "    X_test, y_test = create_sliding_windows(test_scaled, window_size)\n",
    "    \n",
    "    if len(X_test) == 0:\n",
    "        print(\"Test set is empty. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # Reshape\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    \n",
    "    # Tensors\n",
    "    X_train_t = torch.tensor(X_train).float()\n",
    "    y_train_t = torch.tensor(y_train).float()\n",
    "    X_test_t = torch.tensor(X_test).float()\n",
    "    y_test_t = torch.tensor(y_test).float()\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 2. Model Training\n",
    "    model = LSTMModel(\n",
    "        input_size=2, # <-- Remember input_size=2\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    model, _, _ = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        EPOCHS,\n",
    "        LEARNING_RATE, # Use global LR\n",
    "        PATIENCE,\n",
    "        f'models/lstm_model_w{window_size}.pth'\n",
    "    )\n",
    "    \n",
    "    # 3. One-Step-Ahead Evaluation (as decided)\n",
    "    preds = get_one_step_ahead_predictions(model, test_loader)\n",
    "    actuals = y_test_t.cpu().numpy().flatten()\n",
    "    \n",
    "    # 4. Calculate Metrics\n",
    "    metrics = calculate_metrics(preds, actuals)\n",
    "    metrics['window_size'] = window_size\n",
    "    \n",
    "    print(f\"--- Metrics for window {window_size}: {metrics} ---\")\n",
    "    return metrics\n",
    "\n",
    "# --- Run the Experiments ---\n",
    "window_sizes = [10, 25, 50, 100]\n",
    "experiment_results = []\n",
    "\n",
    "# Pass the main 2-feature features_np array\n",
    "for ws in window_sizes:\n",
    "    result = run_window_experiment(ws, features_np) \n",
    "    if result:\n",
    "        experiment_results.append(result)\n",
    "\n",
    "# --- Compare Results ---\n",
    "print(\"\\n\\n{'='*20} FINAL WINDOW SIZE COMPARISON {'='*20}\")\n",
    "results_df = pd.DataFrame(experiment_results).set_index('window_size')\n",
    "display(results_df)\n",
    "\n",
    "# Plot the results\n",
    "results_df.plot(kind='bar', subplots=True, layout=(1, 3), figsize=(18, 5), title=\"LSTM Model Performance by Window Size\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
